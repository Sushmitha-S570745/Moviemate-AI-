{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8W6QNR3xzEO",
        "outputId": "b200cec3-523c-4df0-d0d2-539dcad603cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "%cd gdrive/My Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5jQBSpUA1YJ"
      },
      "source": [
        "Requirements:\n",
        "\n",
        "1. **LangChain**: A framework for developing applications powered by language models.\n",
        "2. **LangChain-Community**: A collaborative space for sharing resources, tools, and discussions related to LangChain.\n",
        "3. **LangChain-OpenAI**: A LangChain extension providing integration with OpenAI's language models.\n",
        "4. **LangChain-Experimental**: A repository for experimental features and prototypes within the LangChain ecosystem.\n",
        "5. **Neo4j**: A graph database management system designed for handling and querying connected data.\n",
        "6. **tiktoken**: A library for tokenizing text, commonly used with language models to preprocess input and output.\n",
        "7. **yfiles_jupyter_graphs**: A library for creating and visualizing graphs and network diagrams in Jupyter notebooks.\n",
        "8. **Streamlit**: An open-source framework for building interactive, web-based applications using Python.\n",
        "9. **Localtunnel**: A tool that allows you to expose your local web server to the internet via a secure tunnel.\n",
        "10. **CTransformers** : A python wrapper for transfomer based models with essential configurations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "_e8mFtjUpsf-"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet  langchain langchain-community langchain-openai langchain-experimental neo4j tiktoken yfiles_jupyter_graphs streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFTCrVRKSfls",
        "outputId": "13228bb9-871a-4ef8-a258-c574343a8f68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ctransformers in /usr/local/lib/python3.11/dist-packages (0.2.27)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from ctransformers) (0.30.2)\n",
            "Requirement already satisfied: py-cpuinfo<10.0.0,>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from ctransformers) (9.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->ctransformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->ctransformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->ctransformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->ctransformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->ctransformers) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->ctransformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->ctransformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->ctransformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->ctransformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->ctransformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->ctransformers) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "pip install ctransformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQpoIlvoWU-Y",
        "outputId": "402c801f-fd51-4917-ab79-a4e2a854e936"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: neo4j in /usr/local/lib/python3.11/dist-packages (5.28.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from neo4j) (2025.2)\n"
          ]
        }
      ],
      "source": [
        "pip install neo4j"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKRc3oenyXvO",
        "outputId": "8dd7ff2c-b1a7-4c8a-bda8-063c7de0506a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K\n",
            "up to date, audited 23 packages in 3s\n",
            "\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K\n",
            "2 \u001b[31m\u001b[1mhigh\u001b[22m\u001b[39m severity vulnerabilities\n",
            "\n",
            "To address all issues (including breaking changes), run:\n",
            "  npm audit fix --force\n",
            "\n",
            "Run `npm audit` for details.\n",
            "\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K"
          ]
        }
      ],
      "source": [
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQtND_fzFRNI"
      },
      "source": [
        "The provided code imports several components from different libraries, each serving a specific purpose. The `RunnableBranch`, `RunnableLambda`, `RunnableParallel`, and `RunnablePassthrough` from LangChain Core are utilities for creating complex workflows involving branching logic, function definitions, parallel execution, and straightforward data passing. For crafting prompts, `ChatPromptTemplate` and `PromptTemplate` are used to design structured inputs for language models.\n",
        "\n",
        "The `BaseModel` and `Field` from Pydantic assist in data validation and configuration, while typing utilities like `Tuple`, `List`, and `Optional` are used for type hinting in Python code. The message classes `AIMessage` and `HumanMessage` define interactions between an AI and a user. The `StrOutputParser` is employed for converting outputs into string format.\n",
        "\n",
        "The `os` module provides a way to interact with the operating system, and `Neo4jGraph` facilitates working with Neo4j databases. `TokenTextSplitter` helps in breaking down text into manageable tokens, essential for language processing. `ChatOpenAI` enables communication with OpenAI's chat models, and `LLMGraphTransformer` is used for transforming graph data with language models. For database interaction, `GraphDatabase` connects with Neo4j, and `GraphWidget` allows for the creation and visualization of graphs in Jupyter notebooks.\n",
        "\n",
        "Further, `Neo4jVector` supports storing and retrieving embeddings in Neo4j, while `OpenAIEmbeddings` provides tools for generating embeddings from OpenAI models. The `remove_lucene_chars` function cleans text by removing specific characters, ensuring data is properly formatted for Neo4j. Lastly, `ConfigurableField` allows for configurable data fields within a runnable, aiding in the customization of workflows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "n5jFoh3cqFfH"
      },
      "outputs": [],
      "source": [
        "# %%writefile main.py\n",
        "\n",
        "from langchain_core.runnables import (\n",
        "    RunnableBranch,\n",
        "    RunnableLambda,\n",
        "    RunnableParallel,\n",
        "    RunnablePassthrough,\n",
        ")\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.prompts.prompt import PromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from typing import Tuple, List, Optional\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "import os\n",
        "from langchain_community.graphs import Neo4jGraph\n",
        "from langchain.text_splitter import TokenTextSplitter\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
        "from neo4j import GraphDatabase\n",
        "from yfiles_jupyter_graphs import GraphWidget\n",
        "from langchain_community.vectorstores import Neo4jVector\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores.neo4j_vector import remove_lucene_chars\n",
        "from langchain_core.runnables import ConfigurableField, RunnableParallel, RunnablePassthrough\n",
        "\n",
        "\n",
        "try:\n",
        "  import google.colab\n",
        "  from google.colab import output\n",
        "  output.enable_custom_widget_manager()\n",
        "except:\n",
        "  pass\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-n8hzmiPGDkr"
      },
      "source": [
        "These lines of code set environment variables for accessing the OpenAI API and a Neo4j database:\n",
        "\n",
        "- `os.environ[\"OPENAI_API_KEY\"] = \"sk-....\"` sets the API key for accessing OpenAI's services.\n",
        "- `os.environ[\"NEO4J_URI\"] = \"bolt+s://d......databases.neo4j.io:7687\"` specifies the URI for connecting to the Neo4j database.\n",
        "- `os.environ[\"NEO4J_USERNAME\"] = \"usr\"` sets the username for the Neo4j database connection.\n",
        "- `os.environ[\"NEO4J_PASSWORD\"] = \"pwd\"` sets the password for the Neo4j database connection.\n",
        "\n",
        "These environment variables are crucial for securely storing sensitive information required for connecting to external services."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "u-x0sDvkzApK"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"#replace your OPEN_API key\n",
        "os.environ[\"NEO4J_URI\"] = \"neo4j+s://06464093.databases.neo4j.io\"#NEO4J URL\n",
        "os.environ[\"NEO4J_USERNAME\"] = \"neo4j\" #NEO4J USERNAME\n",
        "os.environ[\"NEO4J_PASSWORD\"] = \"UjMMz0QsAh59ZpBOlsRaJV0kqX-5Xkec8F7Xp5peD6Y\"#NEO4J PASSWORD\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3SvhzJ4GIM2"
      },
      "source": [
        "This instance, stored in the variable graph, can be used to interact with the Neo4j database. The Neo4jGraph class provides methods and functionalities to execute queries, retrieve data, and manipulate the graph database, facilitating seamless integration and operations within the LangChain framework."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "XwzkR_GOzArj"
      },
      "outputs": [],
      "source": [
        "graph = Neo4jGraph()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfpW5QoAudoX",
        "outputId": "99fdbdcd-b17d-4a09-b8b3-e2b58023421f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tmdbv3api in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.11/dist-packages (1.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from tmdbv3api) (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from wikipedia) (4.13.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->tmdbv3api) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->tmdbv3api) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->tmdbv3api) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->tmdbv3api) (2025.1.31)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia) (4.13.2)\n",
            "‚úÖ Added: A Working Man\n",
            "‚úÖ Added: A Minecraft Movie\n",
            "‚úÖ Added: In the Lost Lands\n",
            "‚úÖ Added: Captain America: Brave New World\n",
            "‚úÖ Added: The Siege\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.11/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Added: G20\n",
            "‚úÖ Added: Novocaine\n",
            "‚úÖ Added: Sinners\n",
            "‚úÖ Added: Gunslingers\n",
            "‚úÖ Added: The Woman in the Yard\n",
            "‚úÖ Added: Conclave\n",
            "‚úÖ Added: Moana 2\n",
            "‚úÖ Added: Bullet Train Explosion\n",
            "‚úÖ Added: Mufasa: The Lion King\n",
            "‚úÖ Added: Sonic the Hedgehog 3\n",
            "‚úÖ Added: The Passion of the Christ\n",
            "‚úÖ Added: Home Sweet Home: Rebirth\n",
            "‚úÖ Added: A Knight's War\n",
            "‚úÖ Added: Cleaner\n",
            "‚úÖ Added: The Codes of War\n",
            "‚úÖ Added: Laila\n",
            "‚úÖ Added: Mickey 17\n",
            "‚úÖ Added: Carjackers\n",
            "‚úÖ Added: Ask Me What You Want\n",
            "‚úÖ Added: The Hard Hit\n",
            "‚úÖ Added: iHostage\n",
            "‚úÖ Added: Flight Risk\n",
            "‚úÖ Added: Sugar Baby\n",
            "‚úÖ Added: Deva\n",
            "‚úÖ Added: The Gorge\n",
            "‚úÖ Added: Locked\n",
            "‚úÖ Added: Cosmic Chaos\n",
            "‚úÖ Added: Batman Ninja vs. Yakuza League\n",
            "‚úÖ Added: Avengers: Infinity War\n",
            "‚úÖ Added: Counterattack\n",
            "‚úÖ Added: Easter Bloody Easter\n",
            "‚úÖ Added: Peter Pan's Neverland Nightmare\n",
            "‚úÖ Added: Ash\n",
            "‚úÖ Added: The Quiet Ones\n",
            "‚úÖ Added: Snow White\n",
            "‚úÖ Added: Here After\n",
            "‚úÖ Added: Turno nocturno\n",
            "‚úÖ Added: The Amateur\n",
            "‚úÖ Added: Gladiator II\n",
            "‚úÖ Added: My Fault\n",
            "‚úÖ Added: Pulp Fiction\n",
            "‚úÖ Added: Fight or Flight\n",
            "‚úÖ Added: Superboys of Malegaon\n",
            "‚úÖ Added: Venom: The Last Dance\n",
            "‚úÖ Added: Deadpool & Wolverine\n",
            "‚úÖ Added: Bloody Escape: Bats out of Hell\n",
            "‚úÖ Added: Memoir of a Snail\n",
            "‚úÖ Added: 825 Forest Road\n",
            "‚úÖ Added: Despicable Me 4\n",
            "‚úÖ Added: Kraven the Hunter\n",
            "‚úÖ Added: The Accountant 2\n",
            "‚úÖ Added: Demon City\n",
            "‚úÖ Added: Warfare\n",
            "‚úÖ Added: Amaran\n",
            "‚úÖ Added: The Good, the Bad and the Ugly\n",
            "‚úÖ Added: Una peque√±a confusi√≥n\n",
            "‚úÖ Added: Flow\n",
            "‚úÖ Added: Plankton: The Movie\n",
            "‚úÖ Added: Companion\n",
            "‚úÖ Added: Popeye the Slayer Man\n",
            "‚úÖ Added: Heart Eyes\n",
            "‚úÖ Added: Spider-Man: Homecoming\n",
            "‚úÖ Added: The Wild Robot\n",
            "‚úÖ Added: Your Fault\n",
            "‚úÖ Added: Thunderbolts*\n",
            "‚úÖ Added: The Substance\n",
            "‚úÖ Added: Nightwatch: Demons Are Forever\n",
            "‚úÖ Added: Vera and the Pleasure of Others\n",
            "‚úÖ Added: Batman v Superman: Dawn of Justice\n",
            "‚úÖ Added: The Lion King\n",
            "‚úÖ Added: Panda Plan\n",
            "‚úÖ Added: Ghost Game\n",
            "‚úÖ Added: GoodFellas\n",
            "‚úÖ Added: Blade Runner 2049\n",
            "‚úÖ Added: I, the Executioner\n",
            "‚úÖ Added: Black Panther\n",
            "‚úÖ Added: Inside Out 2\n",
            "‚úÖ Added: Hellhound\n",
            "‚úÖ Added: Beauty and the Beast\n",
            "‚úÖ Added: Old Guy\n",
            "‚úÖ Added: Black Bag\n",
            "‚úÖ Added: Star Wars\n",
            "‚úÖ Added: Avengers: Endgame\n",
            "‚úÖ Added: Dog Man\n",
            "‚úÖ Added: xXx\n",
            "‚úÖ Added: Red One\n",
            "‚úÖ Added: Interstellar\n",
            "‚úÖ Added: Oh, Canada\n",
            "‚úÖ Added: Star Wars: The Last Jedi\n",
            "‚úÖ Added: Titanic\n",
            "‚úÖ Added: Until Dawn\n",
            "‚úÖ Added: Alarum\n",
            "‚úÖ Added: Guardians of the Galaxy Vol. 2\n",
            "‚úÖ Added: Anora\n",
            "‚úÖ Added: Sky Force\n",
            "\n",
            "üé¨ Total documents prepared: 100\n"
          ]
        }
      ],
      "source": [
        "# ‚úÖ Install required packages\n",
        "!pip install tmdbv3api wikipedia\n",
        "\n",
        "from tmdbv3api import TMDb, Movie\n",
        "import wikipedia\n",
        "from langchain_core.documents import Document\n",
        "import time\n",
        "\n",
        "# Setup TMDB\n",
        "tmdb = TMDb()\n",
        "tmdb.api_key = \"88bd80b2be4bd14d2d4cb3229e072e9f\"\n",
        "tmdb.language = 'en'\n",
        "tmdb.debug = True\n",
        "\n",
        "movie_api = Movie()\n",
        "\n",
        "# Storage for LangChain documents\n",
        "documents = []\n",
        "\n",
        "# Fetch top 100 movies using TMDB pages (20 per page)\n",
        "for page in range(1, 6):  # 5 pages x 20 = 100 movies\n",
        "    top_movies = movie_api.popular(page=page)\n",
        "\n",
        "    for movie in top_movies:\n",
        "        try:\n",
        "            title = movie.title\n",
        "            overview = movie.overview\n",
        "            release_date = movie.release_date\n",
        "            rating = movie.vote_average\n",
        "\n",
        "            # Try to fetch Wikipedia summary\n",
        "            try:\n",
        "                wiki_summary = wikipedia.summary(title)\n",
        "            except:\n",
        "                wiki_summary = \"Wikipedia summary not found.\"\n",
        "\n",
        "            # Combine into one rich document\n",
        "            content = (\n",
        "                f\"Title: {title}\\n\"\n",
        "                f\"Release Date: {release_date}\\n\"\n",
        "                f\"Overview: {overview}\\n\"\n",
        "                f\"Rating: {rating}\\n\"\n",
        "                f\"Wikipedia Summary: {wiki_summary}\"\n",
        "            )\n",
        "\n",
        "            doc = Document(page_content=content, metadata={\"source\": title})\n",
        "            documents.append(doc)\n",
        "\n",
        "            print(f\"‚úÖ Added: {title}\")\n",
        "            time.sleep(1)  # be nice to the Wikipedia API\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to process movie: {movie.title} ‚Äì {str(e)}\")\n",
        "\n",
        "print(f\"\\nüé¨ Total documents prepared: {len(documents)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSG1Hf8vkcjt",
        "outputId": "0441acb8-aa76-4d9a-9776-68109dcf948c"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNSNunP2Gd2c"
      },
      "source": [
        "\n",
        "\n",
        "- **TokenTextSplitter**: This class is used to split text into chunks, typically to prepare it for processing by language models or other natural language processing tasks.\n",
        "- **text_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=24)**: Initializes an instance of `TokenTextSplitter` with a chunk size of 512 tokens and an overlap of 24 tokens between consecutive chunks.\n",
        "- **documents = text_splitter.split_documents(data)**: Splits the input `data` (presumably a collection of text documents) into chunks of the specified size and overlap, storing the resulting chunks in the `documents` variable.\n",
        "\n",
        "This approach is useful for breaking down large text documents or datasets into manageable pieces that can be processed more efficiently by downstream tasks or models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "WSBwTblIxmgO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95aaa468-5ac0-4b61-cc60-19e08539b116"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÑ Total chunks created: 113\n"
          ]
        }
      ],
      "source": [
        "from langchain.text_splitter import TokenTextSplitter\n",
        "\n",
        "text_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=24)\n",
        "split_docs = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"üìÑ Total chunks created: {len(split_docs)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain-openai\n"
      ],
      "metadata": {
        "id": "M1HlYgvPRAYi"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Neo4jVector\n",
        "\n",
        "embeddings = OpenAIEmbeddings(api_key=\"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\")\n",
        "\n",
        "vectorstore = Neo4jVector.from_documents(\n",
        "    documents=split_docs,\n",
        "    embedding=embeddings,\n",
        "    url=\"neo4j+s://06464093.databases.neo4j.io\",\n",
        "    username=\"neo4j\",\n",
        "    password=\"UjMMz0QsAh59ZpBOlsRaJV0kqX-5Xkec8F7Xp5peD6Y\",\n",
        "    index_name=\"movie_rag_index\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "A7KmMCtLQdDW"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2MEovPDGzBW"
      },
      "source": [
        "\n",
        "\n",
        "- **ChatOpenAI**: This class is used to interact with OpenAI's chat models. In this case, it's initialized with parameters like `temperature=0` and `model_name=\"gpt-3.5-turbo-0125\"`.\n",
        "- **LLMGraphTransformer**: This class is used to transform text documents into graph documents using a language model (LLM).\n",
        "- **Neo4jGraph**: This class represents a connection to a Neo4j graph database.\n",
        "\n",
        "\n",
        "The code first initializes the language model (`llm`) and the transformer (`llm_transformer`) to convert text documents (`documents`) into graph documents. Then, it initializes a `Neo4jGraph` instance (`graph`) and adds the transformed graph documents to the Neo4j database using the `add_graph_documents` method.\n",
        "\n",
        "This approach integrates natural language processing with graph database operations, enabling the creation of structured graph representations from unstructured text data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Load vector store (optional if already initialized earlier)\n",
        "vectorstore = Neo4jVector(\n",
        "    embedding=embeddings,\n",
        "    url=\"neo4j+s://06464093.databases.neo4j.io\",\n",
        "    username=\"neo4j\",\n",
        "    password=\"UjMMz0QsAh59ZpBOlsRaJV0kqX-5Xkec8F7Xp5peD6Y\",\n",
        "    index_name=\"movie_rag_index\"\n",
        ")\n",
        "\n",
        "# Create retriever\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# Load OpenAI LLM\n",
        "llm = ChatOpenAI(\n",
        "    temperature=0,\n",
        "    model_name=\"gpt-3.5-turbo-0125\",  # or gpt-4-0125-preview\n",
        "    api_key=\"your-openai-api-key\"\n",
        ")\n",
        "\n",
        "# Build RAG Chain (retrieval + generation)\n",
        "rag_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "TU0z2WkwTJ8F"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts import PromptTemplate\n"
      ],
      "metadata": {
        "id": "R8MYDSn3UmSS"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_prompt = PromptTemplate(\n",
        "    input_variables=[\"chat_history\", \"question\", \"context\"],\n",
        "    template=\"\"\"\n",
        "You are a helpful movie assistant. Use the context below to answer the user's question.\n",
        "Keep it short, relevant, and include movie titles if appropriate.\n",
        "\n",
        "Context: {context}\n",
        "Chat History: {chat_history}\n",
        "Question: {question}\n",
        "\n",
        "Helpful Answer:\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "R_-EgCOyUpZc"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "xdQSpdUaUxoI"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversational_rag_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    memory=memory,\n",
        "    # combine_docs_chain_kwargs={\"prompt\": custom_prompt}  # optional if using custom\n",
        ")\n"
      ],
      "metadata": {
        "id": "3bJ4m2zUU08z"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HHjT9VpwU4Zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tmdbv3api\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hTbZH1jaOov",
        "outputId": "ce681856-65f2-4ef3-9091-7018bdd6974f"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tmdbv3api in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from tmdbv3api) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->tmdbv3api) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->tmdbv3api) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->tmdbv3api) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->tmdbv3api) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip show tmdbv3api\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aC1QuqZZastM",
        "outputId": "335da693-6b47-4316-a338-0b8b9e5c6d09"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: tmdbv3api\n",
            "Version: 1.9.0\n",
            "Summary: A lightweight Python library for The Movie Database (TMDb) API.\n",
            "Home-page: https://github.com/AnthonyBloomer/tmdbv3api\n",
            "Author: Anthony Bloomer\n",
            "Author-email: ant0@protonmail.ch\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: requests\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "EIkFhW5cWU_G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e553ceb6-a2ea-456e-9200-d1e41812fb66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile main.py\n",
        "import os\n",
        "import time\n",
        "from typing import List\n",
        "from langchain_core.documents import Document\n",
        "from langchain.text_splitter import TokenTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_community.vectorstores import Neo4jVector\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from tmdbv3api import TMDb, Movie\n",
        "import wikipedia\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ‚úÖ Load environment variables\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "os.environ[\"NEO4J_URI\"] = \"neo4j+s://06464093.databases.neo4j.io\"\n",
        "os.environ[\"NEO4J_USERNAME\"] = \"neo4j\"\n",
        "os.environ[\"NEO4J_PASSWORD\"] = \"UjMMz0QsAh59ZpBOlsRaJV0kqX-5Xkec8F7Xp5peD6Y\"\n",
        "\n",
        "# ‚úÖ TMDB Setup\n",
        "tmdb = TMDb()\n",
        "tmdb.api_key = \"88bd80b2be4bd14d2d4cb3229e072e9f\"\n",
        "tmdb.language = 'en'\n",
        "tmdb.debug = True\n",
        "\n",
        "movie_api = Movie()\n",
        "documents = []\n",
        "\n",
        "# ‚úÖ Fetch Top 100 Movies (TMDB + Wikipedia)\n",
        "for page in range(1, 6):\n",
        "    top_movies = movie_api.popular(page=page)\n",
        "    for movie in top_movies:\n",
        "        try:\n",
        "            title = movie.title\n",
        "            overview = movie.overview\n",
        "            release_date = movie.release_date\n",
        "            rating = movie.vote_average\n",
        "\n",
        "            try:\n",
        "                wiki_summary = wikipedia.summary(title)\n",
        "            except:\n",
        "                wiki_summary = \"Wikipedia summary not found.\"\n",
        "\n",
        "            content = (\n",
        "                f\"Title: {title}\\n\"\n",
        "                f\"Release Date: {release_date}\\n\"\n",
        "                f\"Overview: {overview}\\n\"\n",
        "                f\"Rating: {rating}\\n\"\n",
        "                f\"Wikipedia Summary: {wiki_summary}\"\n",
        "            )\n",
        "\n",
        "            doc = Document(page_content=content, metadata={\"source\": title})\n",
        "            documents.append(doc)\n",
        "\n",
        "            print(f\"‚úÖ Added: {title}\")\n",
        "            time.sleep(1)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to process movie: {movie.title} ‚Äì {str(e)}\")\n",
        "\n",
        "print(f\"\\nüé¨ Total documents prepared: {len(documents)}\")\n",
        "\n",
        "# ‚úÖ Chunk the documents\n",
        "text_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=24)\n",
        "split_docs = text_splitter.split_documents(documents)\n",
        "print(f\"üìÑ Total chunks created: {len(split_docs)}\")\n",
        "\n",
        "# ‚úÖ Initialize Embeddings\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "# ‚úÖ Store Chunks in Neo4j\n",
        "vectorstore = Neo4jVector.from_documents(\n",
        "    documents=split_docs,\n",
        "    embedding=embeddings,\n",
        "    url=\"neo4j+s://06464093.databases.neo4j.io\",\n",
        "    username=\"neo4j\",\n",
        "    password=\"UjMMz0QsAh59ZpBOlsRaJV0kqX-5Xkec8F7Xp5peD6Y\",\n",
        "    index_name=\"movie_rag_index\"\n",
        ")\n",
        "print(\"‚úÖ Embeddings successfully stored in Neo4j!\")\n",
        "\n",
        "# ‚úÖ Conversational Chain Setup\n",
        "retriever = vectorstore.as_retriever()\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\", temperature=0)\n",
        "\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True,\n",
        "    output_key=\"answer\"  # ‚úÖ THIS FIXES THE ERROR\n",
        ")\n",
        "\n",
        "conversational_rag_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    memory=memory,\n",
        "    return_source_documents=True,\n",
        "    output_key=\"answer\"  # ‚úÖ Match with memory\n",
        ")\n",
        "def answerquery(prompt: str):\n",
        "    result = conversational_rag_chain.invoke({\"question\": prompt})\n",
        "    return result[\"answer\"]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "UzFfyTKBZfoa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5b95952-4e9b-4cc7-a9e4-3d554a7e4f11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from main import answerquery\n",
        "\n",
        "st.set_page_config(page_title=\"MovieMate AI\", page_icon=\"üé¨\")\n",
        "st.title(\"üé¨ MovieMate AI: Your Movie Recommendation Assistant\")\n",
        "\n",
        "# Initialize chat history\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display chat messages from history on app rerun\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "# Use chat-style input\n",
        "if prompt := st.chat_input(\"Ask me anything about movies...\"):\n",
        "    # Display user message\n",
        "    st.chat_message(\"user\").markdown(prompt)\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "    # Get response from RAG chain\n",
        "    response = answerquery(prompt)\n",
        "\n",
        "    # Display assistant response\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        st.markdown(response)\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFaxmm4cZyu9",
        "outputId": "a61309b9-ce92-4442-e7f7-1422af9d8ecd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.245.0.181:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!streamlit run app.py"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}